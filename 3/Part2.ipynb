{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on experiment 2\n",
    "\n",
    "## k-means Clustering with Semidefinite Programming\n",
    "\n",
    "Clustering is an unsupervised machine learning problem in which we try to partition a given dataset into $k$ subsets based on distance between data points or similarity among them. The goal is to find $k$ centers and to assign each data point to one of the centers such that the sum of the square distances between them are minimal [1]. This problem is known to be NP-hard. \n",
    "\n",
    "#### Clustering problem\n",
    "Given a set of $n$ points in a $d-$dimensional Euclidean space, denoted by\n",
    "\\begin{equation*}\n",
    "S = \\{ \\mathbf{s}_i = (s_{i1}, \\cdots, s_{id})^\\top~\\in \\mathbb{R}^d ~~ i = 1, \\cdots, n\\}\n",
    "\\end{equation*}\n",
    "find an assignment of the $n$ points into $k$ disjoint clusters $\\mathcal{S} = (S_1, \\cdots, S_k)$ whose centers are $\\mathbf{c}_j(j = 1, \\cdots, k)$ based on the total sum of squared Euclidean distances from each point $\\mathbf{s}_i$ to its assigned cluster centroid $\\mathbf{c}_i$, i.e.,\n",
    "\\begin{equation}\n",
    "f(S,\\mathcal{S}) = \\sum_{j=1}^k\\sum_{i=1}^{|S_j|}\\|\\mathbf{s}_i^{j} - \\mathbf{c}_j \\|^2,\n",
    "\\label{eq:cluster}\\tag{Clustering Problem}\n",
    "\\end{equation}\n",
    "where $|S_j|$ is the number of points in $S_j$, and $\\mathbf{s}_i^{j} $ is the $i^{th}$ point in $S_j$.\n",
    "\n",
    "#### Lloyd's algorithm for k-means\n",
    "\n",
    "\n",
    "\n",
    "  **1.** Choose initial cluster centers $\\mathbf{c}_1, \\mathbf{c}_2, \\cdots , \\mathbf{c}_k$\n",
    "  \n",
    "  **2.** Repeat until convergence:\n",
    "  $$\\begin{cases}\n",
    "  \\text{Assignment step: }~~  \\mathbf{s}_i ~~\\text{belongs to cluster } j\\text{, where} j := \\text{argmin}_{j \\in [1,k]} \\|\\mathbf{s}_i-\\mathbf{c}_j\\|   \\\\\n",
    "  \\text{Update each cluster center: }~~~~~~~~ \\mathbf{c}_j = \\frac 1{|S_j|} \\sum_{i=1}^{|S_j|} \\mathbf{s}_i^{j} \\\\\n",
    "  \\end{cases}$$\n",
    "\n",
    "\n",
    "Note that the algorithm converges to local optimal points, so\n",
    "\\eqref{eq:cluster} can be arbitrarily bad depending on the\n",
    "initialization of the cluster centers.\n",
    "\n",
    "**SDP relaxation of the problem:** The work (Peng & Wei, 2007)\n",
    "proposes an SDP-relaxation to approximately solve the aforementioned\n",
    "model-free $k-$means clustering problem. The resulting optimization\n",
    "problem (See section (2) of (Peng & Wei, 2007) for details of this\n",
    "    relaxation and Lecture 13 for a brief introduction.) takes the standard semidefinite programming form\n",
    "\n",
    "$$\\mathbf{X}^\\star \\in \\arg \\min_{ \\mathbf{X} } \\bigg\\{ \\langle \\boldsymbol{C}, \\mathbf{X} \\rangle : ~\\underbrace{\\mathbf{X} \\mathbf{1} =  \\mathbf{1}}_{A_1(\\mathbf{X}) = b_1},\\underbrace{~\\mathbf{X}^{\\top} \\mathbf{1} = \\mathbf{1}}_{A_2(\\mathbf{X}) = b_2},~\\underbrace{\\mathbf{X} \\geq 0}_{B(\\mathbf{X}) \\in \\mathcal{K}},~\\underbrace{\\mathrm{Tr}(\\mathbf{X}) \\leq \\kappa, ~\\mathbf{X} \\in \\mathbb{R}^{p\\times p} ,~\\mathbf{X}\\succeq 0 }_{\\mathcal{X}} \\bigg\\},\n",
    "\\label{prob1}\\tag{Problem 1}$$ \n",
    "where $C \\in \\mathbb{R}^{p \\times p}$ is the\n",
    "Euclidean distance matrix between the data points.\n",
    "$\\mathrm{Tr}(\\mathbf{X}) \\leq \\kappa$ enforces approximately low-rank\n",
    "solutions, the linear inclusion constraint $\\mathbf{X} \\geq 0$ is element-wise\n",
    "nonnegativity of ${\\mathbf{X}}$, the linear equality constraints\n",
    "$\\mathbf{X}\\mathbf{1} = \\mathbf{1}$ and $\\mathbf{X}^{\\top} \\mathbf{1} = \\mathbf{1}$\n",
    "require row and column sums of $\\mathbf{X}$ to be equal to 1's, and\n",
    "$\\mathbf{X} \\succeq 0$ means that $\\mathbf{X}$ is positive semi-definite. Recall that\n",
    "$\\mathrm{Tr}(\\mathbf{X}) = \\|\\mathbf{X}\\|_\\ast$ for any positive semi-definite matrix\n",
    "$\\mathbf{X}$.\n",
    "\n",
    "**Algorithm 1.** The SDP in \\eqref{prob1} can be reformulated as $$\n",
    "\\min_{x \\in \\mathcal{X}}~~~ f(x) + g_1(A_1(x)) + g_2(A_2(x)) \\qquad\\text{subject to}\\qquad B(x) \\in \\mathcal{K}, \\label{prob2}\\tag{Equation 2}$$ where $f(x) = \\langle  \\boldsymbol{C}, x \\rangle$ is a\n",
    "smooth convex function, $g_1=\\delta_{\\{b_1\\}}(\\cdot)$ is the indicator\n",
    "function of singleton $\\{b_1\\}$, $g_2=\\delta_{\\{b_2\\}}(\\cdot)$ is the\n",
    "indicator function of singleton $\\{b_2\\}$ and ${\\mathcal{K}}$ is the\n",
    "positive orthant for which computing the projection is easy.\n",
    "\n",
    "Note that the classical Frank-Wolfe method does not apply to this\n",
    "problem due to nonsmooth terms $g_1, g_2$. In the sequel, we will\n",
    "attempt to solve this problem with the HomotopyCGM method proposed in\n",
    "(Yurtsever et al., 2018) to handle the non-smooth problems with a\n",
    "conditional gradient based method.\n",
    "\n",
    "\n",
    "**Algorithm 2.** Another option for solving this problem is Vu-Condat\n",
    "method we have seen in Lecture 12. For solving the problem\n",
    "$ \\begin{equation} \\min_x f(x) + g(A(x)) + h(x) \\end{equation}$ where,\n",
    "$f(x) = \\langle \\boldsymbol{C}, x \\rangle$ and\n",
    "$h(x) = \\delta_{\\mathcal{X}}(x)$. Moreover,\n",
    "$g(A(x)) = g_1(A_1 (x)) + g_2(A_2 (x)) + \\delta_{\\mathcal{K}}(Bx)$ for a\n",
    "suitably defined $g$ and $A$. In particular, define $$\\begin{aligned} \n",
    "z  = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{bmatrix} = \\begin{bmatrix}A_1x \\\\ A_2 x \\\\ Bx \\end{bmatrix},~~~~~A = \\begin{bmatrix} A_1 \\\\ A_2 \\\\ B \\end{bmatrix}~~~~\\Rightarrow~~~~~z= A(x),\n",
    "\\end{aligned}\\tag{Definition of A}$$ and $$\\tag{Definition of g}\n",
    "g(z) = \\delta_{\\{b_1\\}}(z_1) + \\delta_{\\{b_2\\}}(z_2) + \\delta_\\mathcal{K}(z_3).$$\n",
    "Vu-Condat algorithm iterates as follows (recall that $g^\\ast$ denotes\n",
    "the conjugate function of $g$):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions 2.1.1 (5 pts) Characterizing the domain\n",
    "\n",
    "Show that the domain $\\mathcal{X} = \\{\\mathbf{X}: \\mathrm{Tr}(\\mathbf{X}) \\leq \\kappa,~\\mathbf{X} \\in \\mathbb{C}^{p\\times p} ,~\\mathbf{X} \\succeq 0\\}$ is a convex set. For this purpose, apply the definition of set convexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1.2  (10 pts) Gradient of the penalized objective\n",
    "\n",
    "Given a linear inclusion constraint $Tx \\in \\mathcal{Y}$,\n",
    "the corresponding quadratic penalty function is given by\n",
    "$$\\text{QP}_\\mathcal{Y}(x) = \\text{dist}^2(Tx, \\mathcal{Y}) = \\min_{y \\in \\mathcal{Y}} \\| y-Tx\\|^2.$$\n",
    "Write down the constraints in (Equation 2) in\n",
    "the quadratic penalty form and **show that** the penalized objective\n",
    "takes the form\n",
    "$$%\\min_{x \\in \\mathcal{X}} % f(x) + g_{\\beta} (A(x)) + h_{\\beta} (B(x))  = \n",
    "f(x) + \\frac{1}{2\\beta} \\|A_1(x) - b_1\\|^2 + \\frac{1}{2\\beta} \\|A_2(x) - b_2\\|^2 + \\frac{1}{2\\beta}\\text{dist}^2(x, \\mathcal{K}),\\tag{Equation 3}$$\n",
    "and **show that** the gradient of the penalized objective is equal\n",
    "to ${v_k/}{\\beta}$ in the algorithm.\\\n",
    "(Hint: You can write\n",
    "$\\text{dist}^2(Tx, \\mathcal{Y}) = \\| y^\\ast - Tx \\|^2$, where\n",
    "$y^\\ast = \\arg \\min _{y\\in \\mathcal{Y}} \\| y - Tx \\|^2$. and take\n",
    "the derivative with respect to $\\mathbf{X}$ without worrying about\n",
    "$y^\\ast$ depending on $\\mathbf{X}$, thanks to Danskin's theorem\n",
    "(*cf.*, Lecture 10).)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1.3 (5 pts) \n",
    "\n",
    "Write down $v_k$ explicitly by deriving the gradient and projection specific to (Equation 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1.4 (5 pts) \n",
    "\n",
    "Using the definitions we used for $g$ and $A$\n",
    "in (Definition of A)\n",
    "and (Definition of g), show that the $y^{k+1}$ update step of\n",
    "Vu-Condat can be written in the form\n",
    "$$y^{k+1} := \\begin{bmatrix} y_1^{k+1} \\\\ y_2^{k+1} \\\\ y_3^{k+1} \\end{bmatrix} = \\begin{bmatrix} y_1^k \\\\ y_2^k \\\\ y_3^k \\end{bmatrix} +\\sigma \\begin{bmatrix} A_1\\tilde x^{k+1} - b_1 \\\\ A_2\\tilde x^{k+1} - b_2 \\\\ \\tilde x^{k+1} - \\mathrm{proj}_{\\mathcal{K}}(\\sigma^{-1}y_3^k + \\tilde x^{k+1}) \\end{bmatrix},$$\n",
    "and\n",
    "$$A^\\top y^{k+1} = A^\\top y^k + \\sigma (A_1^\\top(A_1 (\\tilde x^{k+1}) - b_1) + A_2^\\top (A_2 (\\tilde x^{k+1}) - b_2) + \\tilde x^{k+1} - \\mathrm{proj}_{\\mathcal{K}}(\\sigma^{-1}y^k_3 + \\tilde x^{k+1})),$$\n",
    "where the vector $y$ in the dual domain can be written in the form\n",
    "$y = [y_1~,~y_2~,~y_3]^\\top$ with $y_1, y_2\\in\\mathbb{R}^p$ and\n",
    "$y_3 \\in\\mathbb{R}^{p^2}$ (see\n",
    "also (Definition of A)).\n",
    "\n",
    "**Hint**: Use Moreau's decomposition to write the update using\n",
    "$\\mathrm{prox}_{g}$ instead of $\\mathrm{prox}_{g^\\ast}$. In\n",
    "particular\n",
    "$$y^{k+1} = \\mathrm{prox}_{\\sigma g^\\ast}(y^k + \\sigma A(\\tilde x^{k+1})) = y^k + \\sigma A( \\tilde x^{k+1}) - \\sigma \\mathrm{prox}_{\\sigma^{-1} g}(\\sigma^{-1}y^k + A(\\tilde x^{k+1})) \\\\\n",
    "%&= y^k + \\sigma (A_1\\tilde x^{k+1} + A_2\\tilde x^{k+1} + \\tilde x^{k+1}) - \\sigma (b_1+b_2+\\mathrm{proj}_\\mathcal{K}(\\sigma^{-1}y^k + \\tilde x^{k+1}))$$\n",
    "The remaining steps are to\n",
    "use (Definition of A) and find how to compute\n",
    "$\\mathrm{prox}_{g}$ when $g$ is in the decomposed form given\n",
    "in (Definition of g)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part2.helpers import *\n",
    "from lib.part2.Llyod_kmeans import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define operators\n",
    "We provide 2 operators and their conjugates:\n",
    "1. `A1`: Linear operator that takes the row sums\n",
    "2. `At2`: Conjugate of operator A1\n",
    "3. `A2`: Linear operator that takes the column sums \n",
    "4. `At2`: Conjugate of operator A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1. Homotopy CGM\n",
    "\n",
    "You will first implement Homotopy CGM which is defined below for the problem specified in (Equation 2).\n",
    "\n",
    "  #### Homotopy Conditional Gradient Method (HCGM)\n",
    "  ---------------------------------------------------------------------------------------------------------------------------------\n",
    "  **1.** Choose $x^0 \\in\\mathcal{X}$ and $\\beta_0 >  0$\n",
    "  \n",
    "  **2.** For $k=1, 2, \\ldots$ perform:\n",
    "  $$\n",
    "  \\begin{cases}\n",
    "  \\gamma_k &= 2/(k+1), ~~\\text{and}~~ \\beta_k = \\beta_0 / \\sqrt{k+1} \\\\\n",
    "  v_k &= \\beta_k \\nabla f(x_k) + A_1^\\top (A_1(x_k) - b_1) + A_2^\\top (A_2(x_k) - b_2) + (x_k - \\text{proj}_{\\mathcal{K}}(x_k))\\\\\n",
    "  \\hat{x}^k &:= \\mathrm{argmin}_{x \\in \\mathcal{X}} \\left \\langle  v_k,  x \\right \\rangle \\\\\n",
    "  x^{k+1} &:= (1-\\gamma_k)x^k + \\gamma_k\\hat{x}^k\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  \n",
    "  **3.** Output:$~x^{k+1}$\n",
    "  \n",
    "  ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "HCGM is designed for solving constrained minimization problems of the form in (Equation 2). We define such a constrained problem in the following cell, where we specify an objective $f$ and a set of penalities $\\{g_i\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.0 (2 pts)\n",
    "\n",
    "Fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Function(\n",
    "    f = lambda X : np.sum(C.flatten()*X.flatten()),\n",
    "    grad = lambda X: ???\n",
    ")\n",
    "\n",
    "g1 = Function(\n",
    "    f = lambda X : np.linalg.norm((A1(X) - b))**2,\n",
    "    grad = lambda X: ???\n",
    ")\n",
    "\n",
    "g2 = Function(\n",
    "    f = lambda X : np.linalg.norm((A2(X) - b))**2,\n",
    "    grad = lambda X: ???\n",
    ")\n",
    "\n",
    "g3 = Function(\n",
    "    f = lambda X : np.sqrt(np.sum(np.minimum(X,0)**2)),\n",
    "    grad = lambda X: 1000*??? # <-- fill in (x_k - proj_K(x_k))\n",
    ")\n",
    "\n",
    "p = ConstrainedProblem(f = f, penalties = [g1, g2, g3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remark**: The 1000 factor in the term for $(x_k - \\text{proj}_{\\mathcal{K}}(x_k))$ in Algorithm 1 is in order to obtain a better practical convergence. This basically corresponds to having different penalty parameters for different constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.1 (4 pts)\n",
    "\n",
    "Complete the missing lines in the function\n",
    "definitions of `HCGM`, which implements Homotopy CGM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implement the LMO. Note that here we have symmetric real matrices. Therefore, you can use `eigsh` from `scipy.sparse.linalg` instead of `svds` which is already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmo(grad, kappa):\n",
    "    # Fill all\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HCGM_state(OptState):\n",
    "    x_k: Matrix\n",
    "    k: int\n",
    "    beta0: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HCGM_initialize(f, Xzero, beta0):\n",
    "    k = 0\n",
    "    return HCGM_state(x_k = Xzero, k = k, beta0 = beta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the update recall that a `ConstrainedProblem` can be unpacked as follows:\n",
    "\n",
    "- `f` which is a `Function` that has a `grad` method\n",
    "- `penalties` which is a list of functions each having a `grad` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HCGM_update(p : ConstrainedProblem, state : HCGM_state):\n",
    "    f, penalties = p\n",
    "    x_k, k, beta0 = state\n",
    "    \n",
    "    gamma_k = ???\n",
    "    beta_k = ???\n",
    "    \n",
    "    v_k = ???\n",
    "    \n",
    "    X_bar = ???\n",
    "    \n",
    "    X_next = ???\n",
    "    \n",
    "    return HCGM_state(x_k = X_next, k = k+1, beta0 = beta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCGM = OptAlgorithm(name=\"HCGM\", init_state = HCGM_initialize, state_update = HCGM_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.2 (1 pts) Run HCGM\n",
    "\n",
    "Run `HCGM` for $5000$-iterations and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HCGM, f1_HCGM, f2_HCGM, f3_HCGM, obj_HCGM, iter_HCGM, time_HCGM = run_HCGM(HCGM, p, maxit=5000, beta0=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(iter_HCGM, f1_HCGM,f2_HCGM, obj_HCGM, X_HCGM, opt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2. Vu-Condat\n",
    "\n",
    "Recall the Vu-Condat algorithm can be used to solve the problem\n",
    "$$ \\begin{equation} \\min_x f(x) + g(A(x)) + h(x) \\end{equation}$$ where,\n",
    "$f(x) = \\langle \\boldsymbol{C}, x \\rangle$ and\n",
    "$h(x) = \\delta_{\\mathcal{X}}(x)$. Moreover,\n",
    "$g(A(x)) = g_1(A_1 (x)) + g_2(A_2 (x)) + \\delta_{\\mathcal{K}}(Bx)$ for a\n",
    "suitably defined $g$ and $A$. In particular, define $$\\begin{aligned} \n",
    "z  = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{bmatrix} = \\begin{bmatrix}A_1x \\\\ A_2 x \\\\ Bx \\end{bmatrix},~~~~~A = \\begin{bmatrix} A_1 \\\\ A_2 \\\\ B \\end{bmatrix}~~~~\\Rightarrow~~~~~z= A(x),\n",
    "\\end{aligned}\\tag{Definition of A}$$ and $$\\tag{Definition of g}\n",
    "g(z) = \\delta_{\\{b_1\\}}(z_1) + \\delta_{\\{b_2\\}}(z_2) + \\delta_\\mathcal{K}(z_3).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.3 (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Vu-Condat we need to specify the proximal operator of $h$ and $g^*$ as well as the linear operator $A$. We do so in the following cell. Note that the `DualVariable` class is created to keep track of all three dual variable $(y_1,y_2,y_3)$ in a concise way.\n",
    "In the following cells fill in the proximal operators of $h$ and $g$.\n",
    "\n",
    "**Hint**: Recall that the proximal operator of an indicator function is a projection. We give you `projSDP(X,kappa)` and (global variable) `kappa`, which projects onto $\\mathcal X$. Further, the proximal operator of a seperable sum is a concatenation of the proximal operators of each term (see the lecture on proximal methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Function(\n",
    "    f = lambda X : np.sum(C.flatten()*X.flatten()),\n",
    "    grad = lambda X : C\n",
    ")\n",
    "\n",
    "h = Function(\n",
    "    f = lambda X : 0 if (ispsd(X) and np.trace(X) <= kappa) else float('inf') ,\n",
    "    prox = lambda tau, X: ???\n",
    ")\n",
    "\n",
    "g = Function(\n",
    "    f = lambda y: 0 if (y.y_1==b) and (y.y_2==b) and (np.all(y.y_3 >=0)) else float('inf'),\n",
    "    prox = lambda sigma, y: DualVariable(???, ???, ???)\n",
    ")\n",
    "\n",
    "def A(x):\n",
    "    return DualVariable(A1(x), A2(x), x)\n",
    "\n",
    "def A_T(y):\n",
    "    return At1(y.y_1)+ At2(y.y_2)+ y.y_3\n",
    "\n",
    "composite = (f, g, h, A, A_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the problem formulation in place we can specify the Vu-Condat algorithm.\n",
    "\n",
    "#### Vu-Condat method\n",
    "  ------------------------------------------------------------------------------------------------------------------------------\n",
    "  **1.** Choose $x^0 \\in\\mathcal{X}, y^0\\in\\mathbb{R}^{2p+p^2}$ and $\\tau, \\sigma >  0$\n",
    "  \n",
    "  **2.** For $k=1, 2, \\ldots$ perform:\n",
    "  $$\\begin{cases}\n",
    "  x^{k+1} &= \\mathrm{prox}_{\\tau h}(x^k - \\tau (\\nabla f(x^k) + A^\\top y^k)) \\\\\n",
    "  \\bar x^{k+1} &= 2x^{k+1} - x^k \\\\\n",
    "  y^{k+1} &= \\mathrm{prox}_{\\sigma g^\\ast}(y^k + \\sigma A(\\bar x^{k+1}))\n",
    "  %\\gamma_k &= 2/(k+1), ~~\\text{and}~~ \\beta_k = \\beta_0 / \\sqrt{k+1} \\\\\n",
    "  %v_k &= \\beta_k \\nabla f(x_k) + A_1^\\top (A_1x_k - b_1) + A_2^\\top (A_2x_k - b_2) + (x_k - \\text{proj}_{\\mathcal{K}}(x_k))\\\\\n",
    "  %\\hat{x}^k &:= \\mathrm{argmin}_{x \\in \\mathcal{X}} \\left \\langle  v_k,  x \\right \\rangle \\\\\n",
    "  %x^{k+1} &:= (1-\\gamma_k)x^k + \\gamma_k\\hat{x}^k\n",
    "  \\end{cases}$$ \n",
    "  \n",
    "  **3.Output: $~x^k$**\n",
    "  \n",
    "  ---------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.4 (4 pts)\n",
    "\n",
    "Complete the missing lines in the function\n",
    "definitions of `PDHG`, which implements the Vu-Condat method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:** \n",
    "\n",
    "- In our case, we advise you to **multiply the step-size for $y_3$ by $10^4$** to obtain a better practical convergence. It is possible to use different dual step sizes, $\\{ \\sigma_1 , \\sigma_2, \\sigma_3 \\}$.  Such a diagonal variable stepsize is supported through `[sigma1, sigma2, sigma3]*dual_variable`.\n",
    "- The algorithm needs the proximal operator of $g^*$, the Fenchel dual of $g$, using the Moreau identity express the prox of $g^*$ in terms of the prox of $g$. You will need to multiply by the stepsize `Sigma` and its inverse `Sigma_inv` which we provide in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDHG_state(OptState):\n",
    "    x_k: Matrix\n",
    "    y_k: DualVariable(Vector, Vector, Matrix)\n",
    "    tau: float\n",
    "    sigma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDHG_initialize(composite, x_zero):\n",
    "    L = 1e2\n",
    "    tau = 1/L\n",
    "    return PDHG_state(x_k = x_zero, y_k = A(x_zero), tau=tau, sigma=1/(L**2*tau))\n",
    "\n",
    "def PDHG_update(composite, state):\n",
    "    f, g, h, A, A_T = composite\n",
    "    y_k = state.y_k\n",
    "    x_k, _, tau, sigma = state\n",
    "    \n",
    "    next_x_k = ???\n",
    "\n",
    "    x_bar = ???\n",
    "    \n",
    "    Sigma = [sigma, sigma, sigma*1e4]\n",
    "    Sigma_inv = [1/sigma, 1/sigma, 1/(sigma*1e4)]\n",
    "    next_y_k = ???\n",
    "    \n",
    "    return PDHG_state(next_x_k, next_y_k, tau, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDHG = OptAlgorithm(name=\"PDHG\", init_state = PDHG_initialize, state_update = PDHG_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.5 (1 pts) Run Vu-Condat\n",
    "\n",
    "Run `PDHG` for $1000$-iterations and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PDHG, f1_PDHG, f2_PDHG, obj_PDHG, iter_PDHG, time_PDHG = run_PDHG(PDHG, composite, maxit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func(iter_PDHG, f1_PDHG,f2_PDHG, obj_PDHG, X_PDHG, opt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing HCGM and Vu-Condat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.6 (3 pt)\n",
    "\n",
    "Plot the results with the function `plot_comp`. What are the final objective values? Are they below the optimal value provided to you in `opt_val`? If yes, explain the reason. Answer in the box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = (time_PDHG, time_HCGM)\n",
    "feas1 = (f1_PDHG, f1_HCGM)\n",
    "feas2 = (f2_PDHG, f3_HCGM)\n",
    "obj   = (obj_PDHG, obj_HCGM)\n",
    "plot_comp(times, feas1,feas2, obj, opt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Rounding: Get the assignments from the result of the SDP\n",
    "Getting the assignments requires going back to the $10$ dimensional space discussed before, and using the coordinates multiplied with the obtained matrix to construct a \"denoised\" version of the data points. This allows then to find the clusters from these $10$ dimensional data. Our implementation is the python reimplementation of their matlab code which can be found on [github](https://github.com/solevillar/kmeans_sdp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_HCGM, assign_HCGM = sdp_rounding(X_HCGM,10, digits)\n",
    "center_PDHG, assign_PDHG = sdp_rounding(X_PDHG,10, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means value: HCGM & Vu-Condat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `value_kmeans`, run the cell below to obtain the $k$-means value before and after running both algorithms. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_before = value_kmeans(digits, labels-1) # k_means value with true labels\n",
    "k_means_after_HCGM  = value_kmeans(digits, assign_HCGM) # k_means value with assigned lables\n",
    "k_means_after_PDHG  = value_kmeans(digits, assign_PDHG) # k_means value with assigned lables\n",
    "\n",
    "print('k-means value initial: {:.4f}'.format(k_means_before))\n",
    "print('k-means value for HCGM: {:.4f}'.format(k_means_after_HCGM))\n",
    "print('k-means value for Vu-Condat: {:.4f}'.format(k_means_after_PDHG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2.7 (3 pts) k-means value: Lloyd's algorithm\n",
    "\n",
    "Run the function `kmeans` a few times and report the $k$-means value obtained by Llyod's algorithm. Compare it with the ones obtained by rounding the solution of convex methods `HCGM` and `PDHG`.  Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_Lloyd, classifications_Lloyd, k_means_Lloyd = kmeans(digits.T, 10)#k_means value with Lloyds k-means algorithm\n",
    "\n",
    "print('k-means value for Lloyd''s algorithm: {:.4f}'.format(k_means_Lloyd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Additional results for clustering fMNIST Data\n",
    "\n",
    "### Misclassification rates: HCGM & Vu-Condat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset:** We use the fashion-MNIST data in (Xiao et al. 2017) which is released as\n",
    "a possible replacement for the MNIST handwritten digits . Each data\n",
    "point is a 28x28 grayscale image, associated with a label from 10\n",
    "classes. Classes\n",
    "are labeled from 0 to 9. First, we extract the meaningful features from\n",
    "this dataset using a simple 2 layers neural network with a sigmoid\n",
    "activation. Then, we apply neural network to 1000 test samples from the\n",
    "same dataset, which gives us a vector $\\mu \\in \\mathbb{R}^{10}$ where\n",
    "each entry represents the probability being in that class. Then, we form\n",
    "the pairwise distance matrix $\\boldsymbol{C}$ by using this probability\n",
    "vectors (In the code, you do not need to worry about any of the processing\n",
    "    details mentioned here. You are directly given the matrix\n",
    "    $\\mathbf{C}$.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Misclassification rate for HCGM: {:.4f}'.format(misclassification_rate(assign_HCGM, labels)))\n",
    "print('Misclassification rate for Vu-Condat: {:.4f}'.format(misclassification_rate(assign_PDHG, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize samples and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_samples(assignment, images, labels):\n",
    "    assignment=assignment.astype(int)\n",
    "    classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "    labels = labels-1\n",
    "    rand_samp = np.random.randint(0,1000,25)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    for i,samp in enumerate(rand_samp):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.imshow(1-np.reshape(images[samp],[28,28]), cmap=plt.cm.gray)\n",
    "        plt.title('Pred. {0}\\n Orig. {1}'.format(classes[assignment[samp].item()],classes[labels[samp].item()]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_samples(assign_HCGM, images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_samples(assign_PDHG, images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "Xiao, Rasul & Vollgraf 2017,  Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\n",
    "\n",
    "Peng & Wei 2007, Approximating K-means-type clustering via semidefinite programming\n",
    "\n",
    "Yurtsever, Fercoq, Locatello & Cevher 2018, A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
