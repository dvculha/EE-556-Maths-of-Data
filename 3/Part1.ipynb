{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c75611-f49c-45ea-b301-772fc5246f8d",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2022\n",
    "\n",
    "In this homework we will solve constrained optimization problems.\n",
    "If you worked with somebody else on this homework, fill in their names here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1618cf-9812-4c59-9c46-e907fdce57f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "701fa6b5-6922-4707-94ea-39701ae38ee1",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d6239-c320-42c7-a6b9-bdaa3677edd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>Warning</b> You will need a couple of libraries for this homework, which you can install with pip by executing the block below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc1fa5-db8d-4358-aea0-dffa8ba8c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4132675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dab2f-86fa-4dc2-b556-12fe8abbc589",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997e534",
   "metadata": {},
   "source": [
    "We have seen in the lectures that given an optimization task whose iterates live on a  convex subset $\\mathcal{X} \\subset \\mathbb{R}^n$ we can ensure the iterates of our algorithms stay within $\\mathcal{X}$ in one of two ways. \n",
    "\n",
    "In the first way, we use projections, computed via the proximal operator $\\text{prox}_{\\delta_\\mathcal{X}}$. In the second way, we use linear minimization oracles  $\\text{lmo}_\\mathcal{X}$ within a conditional gradient framework\n",
    "that take simplicial combinations of elements from the set X whereby producing iterates remaining in $\\mathcal{X}$.\n",
    "\n",
    "The following exercises will help you understand what kind of computations are involved for each of these two operators, and how their computational complexity compares. For this we will work with $\\mathcal{X}$ being the set of low-rank matrices defined via the nuclear norm ball $\\mathcal{X}=\\lbrace X:X\\in \\mathbb{R}^{p\\times n}, \\Vert X  \\Vert_* \\leq \\kappa \\rbrace$ with $\\kappa$ being the radius of the zero-centered nuclear norm ball.\n",
    "\n",
    "We will first mathematically study properties of  the proximal operator $\\text{prox}_{\\delta_\\mathcal{X}}$  (**1.1** ) and the linear minimization oracles $\\text{lmo}_\\mathcal{X}$ (**1.2**) onto this $\\mathcal{X}$ and then compare them empirically by implementing movie recommender system (**1.3**) and image deblurring (**1.4**) algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34393988",
   "metadata": {},
   "source": [
    "## 1.1 Computing projections onto $\\mathcal{X}$\n",
    "\n",
    "\n",
    "#### Question 1.1.1 (2 pts)\n",
    "\n",
    "Recall that given a set $\\mathcal{X} \\subset \\mathbb{R}^{p \\times m}$, its corresponding\n",
    "    projection operator is given by\n",
    "    $\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z}) = \\mathop{\\mathrm{arg\\,min}}\\limits_{{\\bf X}\\in \\mathcal{X}}\\{ \\|{\\bf X}- \\boldsymbol{Z}\\|_F^2\\}, \\; \\forall \\boldsymbol{Z}\\in  \\mathbb{R}^{p \\times m}$.\n",
    "    Using the definition of the proximal operator given in class, show\n",
    "    the equivalence between the projection operator and the proximal\n",
    "    operator:\n",
    "    $$\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})  = \\mathrm{prox}_{\\delta_{\\mathcal{X}}}(\\boldsymbol{Z}),$$\n",
    "    where $\\delta_{\\mathcal{X}}$ is the indicator function of\n",
    "    $\\mathcal{X}$ i.e.  $\\delta_{\\mathcal{X}}({\\bf Y}) = \\begin{cases} 0, \\text{ if } {\\bf Y}\\in \\mathcal{X} \\\\ +\\infty, \\text{ o.w. } \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b900b",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cda554",
   "metadata": {},
   "source": [
    "#### Question 1.1.2 (3 pts)\n",
    "\n",
    "The projection operator of convex sets has an interesting and useful property: it is non-expansive. Mathematically, we write:\n",
    "$$\\begin{align}\n",
    "        \\|\\mathrm{proj}_{\\mathcal{X}} (\\textbf{x}) - \\mathrm{proj}_{\\mathcal{X}} (\\textbf{y})\\| \\leq \\|\\textbf{x}- \\textbf{y}\\|, \\forall \\textbf{x}, \\textbf{y}\\in \\mathbb{R}^p,\\tag{1}\\label{eq:non-expansivity}\n",
    "\\end{align}$$\n",
    "where $\\|\\cdot\\|$ denotes the usual Euclidean norm\n",
    "and $\\mathcal{X}$ is a non-empty, closed and convex set. For keeping\n",
    "things simple, in this point we use the space of vectors\n",
    "$\\mathbb{R}^p$ in which $\\mathcal{X} \\in \\mathbb{R}^p$ is a closed\n",
    "convex set.\n",
    "\n",
    "Informally, \\eqref{eq:non-expansivity} means that the distance between the\n",
    "*projections* of the two points onto $\\mathcal{X}$ will be *no\n",
    "greater* than the distance between the points themselves.\n",
    "Conversely, for non-convex sets, this does not hold (you can try\n",
    "building a counterexample for a doughnut-shaped set).\n",
    "\n",
    "Prove inequality \\eqref{eq:non-expansivity} starting from the equivalent\n",
    "characterization of the Euclidean projection:\n",
    "$\\textbf{z}^* = \\mathrm{proj}_{\\mathcal{X}} (\\textbf{x}) \\iff \\langle \\textbf{x}- \\textbf{z}^*, \\textbf{z}- \\textbf{z}^*\\rangle \\leq 0, \\, \\forall \\textbf{z}\\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d532c",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b88e3",
   "metadata": {},
   "source": [
    "#### Question 1.1.3.  (4 pts)\n",
    "\n",
    "Let ${\\boldsymbol Z}= {\\boldsymbol U}\\boldsymbol{\\Sigma} {\\boldsymbol V}^\\top$\n",
    "    be the singular value decomposition of\n",
    "    ${\\boldsymbol Z}\\in \\mathbb{R}^{p \\times m}$. Denote the diagonal of\n",
    "    $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{s \\times s}$ by a vector\n",
    "    $\\sigma \\in \\mathbb{R}^{s}$, where $s = \\min \\{ p, m \\}$. Let\n",
    "    $\\sigma^{\\ell_1}$ be the projection of $\\sigma$ onto the\n",
    "    $\\ell_1$-norm ball\n",
    "    $\\{ \\textbf{x}: \\textbf{x}\\in \\mathbb{R}^{s} , \\left\\Vert  \\textbf{x} \\right\\Vert_1 \\leq \\kappa  \\}$\n",
    "    with radius $\\kappa$. Show that the projection of this matrix onto\n",
    "    the nuclear norm ball\n",
    "    $\\mathcal{X} = \\{ {\\boldsymbol X}: {\\boldsymbol X}\\in \\mathbb{R}^{p \\times m} , \\| {\\boldsymbol X}\\|_\\ast \\leq \\kappa \\}$\n",
    "    can be computed by projecting $\\sigma$ onto the $\\ell_1$ norm ball,\n",
    "    i.e.,\n",
    "    $$\\mathrm{proj}_\\mathcal{X} ({\\boldsymbol Z}) = {\\boldsymbol U}\\boldsymbol\\Sigma^{\\ell_1} {\\boldsymbol V}^\\top,$$\n",
    "    where $\\Sigma^{\\ell_1} \\in \\mathbb{R}^{s \\times s}$ denotes the\n",
    "    diagonal matrix with diagonal $\\sigma^{\\ell_1}$.\n",
    "\n",
    "(Hint: Use Mirsky's inequality:\n",
    "    $\\| {\\boldsymbol X}- {\\boldsymbol Z}\\|_F \\geq \\| \\boldsymbol{\\Sigma}_{{\\boldsymbol X}} - \\boldsymbol{\\Sigma}_{{\\boldsymbol Z}}\\|_F$,\n",
    "    where\n",
    "    $\\boldsymbol{\\Sigma}_{{\\boldsymbol X}}, \\boldsymbol{\\Sigma}_{{\\boldsymbol Z}} \\in \\mathbb{R}^{s \\times s}$\n",
    "    are the diagonal matrices of the singular values of\n",
    "    ${\\boldsymbol X}, {\\boldsymbol Z}$ respectively.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d656",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dfb3f8",
   "metadata": {},
   "source": [
    "## 1.2 Computing the linear minimization oracle of $\\mathcal{X}$\n",
    "\n",
    "#### Question 1.2.1 (4 pts)\n",
    "\n",
    "Problem 1.1 shows that projection onto the nuclear norm ball requires\n",
    "computing the singular value decomposition. The computational complexity\n",
    "of the singular value decomposition is $\\mathcal{O}(\\min(m^2p,mp^2))$,\n",
    "which can easily become a computational bottleneck if $m$ or $p$ are\n",
    "large. This bottleneck increased the popularity of algorithms that\n",
    "leverage the linear minimization oracle (lmo) instead (e.g.,\n",
    "[Jaggi2013](https://proceedings.mlr.press/v28/jaggi13.html), [yurtsever2018](http://proceedings.mlr.press/v80/yurtsever18a)):\n",
    "$$\\text{lmo}_{\\mathcal{X}}({\\boldsymbol Z})  = \\arg \\min_{{\\boldsymbol X}\\in \\mathcal{X}} \\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle \\qquad \\text{where}\\qquad \\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle = \\text{Tr}({\\boldsymbol Z}^\\top{\\boldsymbol X}).$$\n",
    "Note that $\\text{lmo}_\\mathcal{X}({\\boldsymbol Z})$ is not single valued\n",
    "in general. With abuse of terminology, when we say that we compute the\n",
    "lmo, we actually mean that we compute an instance ${\\boldsymbol X}$ such\n",
    "that ${\\boldsymbol X}\\in \\text{lmo}_\\mathcal{X}({\\boldsymbol Z})$.\n",
    "\n",
    "Show that the lmo$_\\mathcal{X}$ when $\\mathcal{X}$ is the nuclear norm\n",
    "ball:\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: {\\boldsymbol X}\\in \\mathbb{R}^{p \\times m} , \\| {\\boldsymbol X}\\|_\\ast \\leq \\kappa \\}$\n",
    "gives the following output:\n",
    "$$-\\kappa ~ \\! \\mathbf{u}\\mathbf{v}^T   \\in  \\text{lmo}_{{\\mathcal{X}}}({\\boldsymbol Z}) ,$$\n",
    "where $\\mathbf{u}$ and $\\mathbf{v}$ are the left and right singular\n",
    "vectors that correspond to the largest singular value of\n",
    "${\\boldsymbol Z}$.\n",
    "\n",
    "(Hint: By definition\n",
    "$\\kappa ~ \\! \\mathbf{u}\\mathbf{v}^T \\in \\mathcal{X}$. You just need to\n",
    "show\n",
    "$\\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle \\geq \\langle -\\kappa ~ \\! \\mathbf{u}\\mathbf{v}^T,{\\boldsymbol Z}\\rangle$\n",
    "for all ${\\boldsymbol X}\\in \\mathcal{X}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f1d86",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5cc02",
   "metadata": {},
   "source": [
    "## 1.3 Comparing the scalability of $\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})$ and $\\mathrm{lmo}_{\\mathcal{X}} (\\boldsymbol{Z})$\n",
    "\n",
    "In this exercise, we will compare the execution time of\n",
    "$\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})$ and\n",
    "$\\mathrm{lmo}_{\\mathcal{X}} (\\boldsymbol{Z})$ on two datasets provided\n",
    "to you in the codes. These datasets consist of the ratings given by\n",
    "MovieLens users to movies in a given list. The 100k dataset consists of\n",
    "100,000 ratings from 1000 users on 1700 movies. The 1M dataset consists\n",
    "of 1 million ratings from 6000 users on 4000 movies.\n",
    "\n",
    "As you likely figured out already from the numbers above, users do not\n",
    "rate all of the movies, and therefore, we model the ratings as entries\n",
    "of a low-rank matrix, where rows correspond to different users and\n",
    "columns correspond to different movies. A classical task in machine\n",
    "learning is to predict the value of the missing entries, which is called\n",
    "the matrix completion problem.\n",
    "\n",
    "Many other tasks can be formulated as convex minimization problems,\n",
    "constrained to the nuclear-norm ball, which captures a low rank model\n",
    "since it is the atomic norm of rank-1 matrices (see Lecture 4). A good\n",
    "optimization algorithm must ensure feasibility in a scalable way: For\n",
    "instance, the famous Netflix competition data consists of 100480507\n",
    "ratings that 480189 users gave to 17770 movies (much bigger than the\n",
    "datasets above). Projecting a matrix of this size onto the nuclear-norm\n",
    "ball is indeed demanding.\n",
    "\n",
    "#### Question 1.3.1 (4 pts)\n",
    "\n",
    "Implement the projection operator as a function called `proj_nuc` below. You can use the helper function `proj_L1` we define here from the `projL1.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.projL1 import projL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267714eb",
   "metadata": {},
   "source": [
    "Set $\\kappa = 5000$ and measure the computation time of the\n",
    "    projection operator with the 100k and the 1M MovieLens dataset using our provided helper code, which loads the datasets, constructs the data matrix, and times the evaluation of\n",
    "    the projection operator. Write the values you get in a markdown cell.\n",
    "    Run and report the average timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8858303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_nuc(Z, kappa):\n",
    "    \"\"\" This function implements the projection onto the nuclear norm ball.\n",
    "    \"\"\"\n",
    "  \n",
    "    proj = ???\n",
    "    \n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89219f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_completion(\"100k_MovieLens\", proj_nuc, kappa=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce991cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This one can take few minutes!\n",
    "eval_completion(\"1M_MovieLens\", proj_nuc, kappa=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88210fc",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc59c97",
   "metadata": {},
   "source": [
    "#### Question 1.3.2 (4 pts)\n",
    "\n",
    "Implement the lmo with ${\\mathcal{X}}$ as a function called\n",
    "    `lmo_nuc`  below. Set again $\\kappa = 5000$ and measure the\n",
    "    computation time for the 100k and 1M Movielens datasets. Compare these values with\n",
    "    the computation time of the projection operator. You are allowed to use `scipy` (the library is already loaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmo_nuc(Z, kappa):\n",
    "    \"\"\" This function implements the lmo operator for the nuclear norm ball constraint.\n",
    "    \"\"\"\n",
    "    \n",
    "    Zhat = ???\n",
    "    \n",
    "    return Zhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc756d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_completion(\"100k_MovieLens\",lmo_nuc,kappa=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_completion(\"1M_MovieLens\",lmo_nuc,kappa=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb1296",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53be50",
   "metadata": {},
   "source": [
    "## 1.4 Frank-Wolfe for blind image deblurring\n",
    "\n",
    "You are working with the local police to help identify a license plate\n",
    "of a car involved in a crime scene investigation. Unfortunately, the\n",
    "CCTV image of the car is blurry. In this exercise, we simulate this\n",
    "scenario with a deblurred license plate image found from the\n",
    "internet.\n",
    "\n",
    "Deblurring is an instance of the blind deconvolution problem: Given two\n",
    "unknown vectors $\\textbf{x},  {\\boldsymbol w}\\in \\mathbb{R}^L$, we\n",
    "observe their circular convolution\n",
    "$\\textbf{y}=  {\\boldsymbol w}*\\textbf{x}$, i.e.,\n",
    "$$y_\\ell = \\sum_{\\ell'=1}^L w_{\\ell'} x_{\\ell - \\ell' + 1},$$ where the\n",
    "index $\\ell - \\ell' + 1$ in the sum is understood to be modulo $L$.\n",
    "\n",
    "Blind deconvolution seeks to separate ${\\boldsymbol w}$ and\n",
    "$\\textbf{x}$, given $\\textbf{y}$. The operative word *blind* comes from\n",
    "the fact that we do not have much prior information about the signals.\n",
    "In this case, what we can assume is that ${\\boldsymbol w}$ and\n",
    "$\\textbf{x}$ belong to *known* subspaces of $\\mathbb{R}^L$ of dimension\n",
    "$K$ and $N$, i.e., we write $$\\begin{aligned}\n",
    "{\\boldsymbol w}&= {\\boldsymbol B}{\\boldsymbol h}, \\quad {\\boldsymbol h}\\in \\mathbb{R}^K \\\\\n",
    "\\textbf{x}&= {\\boldsymbol C}{\\boldsymbol m}, \\quad {\\boldsymbol m}\\in \\mathbb{R}^N\n",
    "\\end{aligned}$$ for some $L \\times K$ matrix ${\\boldsymbol B}$ and\n",
    "$L \\times N$ matrix ${\\boldsymbol C}$. The columns of these matrices\n",
    "form bases for the subspaces in which ${\\boldsymbol w}$ and $\\textbf{x}$\n",
    "live.\n",
    "\n",
    "As we have seen in Homework 1, natural images have sparse wavelet\n",
    "expansions. Hence, the image $\\textbf{x}$ can be expressed as\n",
    "$\\textbf{x}= {\\boldsymbol C}{\\boldsymbol m}$ with ${\\boldsymbol C}$ is\n",
    "the matrix formed by a subset of the columns of the wavelet transform\n",
    "matrix. In addition, the blur kernel ${\\boldsymbol w}$ is typically due\n",
    "to simple or \"sparse\" motion, which can be written as\n",
    "${\\boldsymbol w}= {\\boldsymbol B}{\\boldsymbol h}$ with ${\\boldsymbol B}$\n",
    "is the matrix formed by a subset of the columns of the identity matrix.\n",
    "\n",
    "In deblurring, $\\textbf{x}$ corresponds to the image we want to recover\n",
    "(i.e., the license plate) and ${\\boldsymbol w}$ to a 2D blur kernel.\n",
    "Thus, the 2D convolution $\\textbf{y}=  {\\boldsymbol w}*\\textbf{x}$\n",
    "produces a blurred image. We assume that we know or can estimate the\n",
    "support of the blur kernel (i.e., the location of its nonzero elements).\n",
    "In real applications, the support can be estimated by an expert using\n",
    "the physical information such as the distance of object to the focus and\n",
    "the camera, the speed of the camera and/or the object, camera shutter\n",
    "speed, etc.\n",
    "\n",
    "In this experiment, we use a very rough estimate for the support - a box\n",
    "at the center of the domain, whose size we have roughly tuned.\n",
    "Interestingly, it is possible to make the plate readable even in this\n",
    "setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b388a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import setup_show\n",
    "setup_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5816b8",
   "metadata": {},
   "source": [
    "###  Reformulating the problem\n",
    "\n",
    "We now reformulate the blind image deconvolution problem, so that we can\n",
    "apply the constrained optimization algorithms we have seen in the\n",
    "course. Let ${\\boldsymbol b}$ be the $L$-point normalized discrete\n",
    "Fourier transform (DFT) of the observation $\\textbf{y}$, i.e,\n",
    "${\\boldsymbol b}= \\mathbf{F} \\textbf{y}$, where $F$ is the DFT matrix.\n",
    "Then, ${\\boldsymbol b}$ can be written as\n",
    "${\\boldsymbol b}= {\\boldsymbol A}({\\bf X})$ where\n",
    "${\\bf X}= {\\boldsymbol h}{\\boldsymbol m}^\\top$ and ${\\boldsymbol A}$ is\n",
    "a linear operator. Explicit expression of this linear operator\n",
    "${\\boldsymbol A}$ is out of the scope of this homework, c.f.,\n",
    "[Ahmed2014](https://ieeexplore.ieee.org/document/6680763/) for further details. This reformulation allows us to\n",
    "express $\\textbf{y}$, which is a nonlinear combination of the\n",
    "coefficients of ${\\boldsymbol h}$ and ${\\boldsymbol m}$, as a linear\n",
    "combination of the entries of their outer product\n",
    "${\\bf X}= {\\boldsymbol h}{\\boldsymbol m}^\\top$. Note that given\n",
    "${\\boldsymbol B}$ and ${\\boldsymbol C}$, recovering ${\\boldsymbol m}$\n",
    "and ${\\boldsymbol h}$ from ${\\boldsymbol b}$ is the same as recovering\n",
    "$\\textbf{x}$ and ${\\boldsymbol w}$ from $\\textbf{y}$.\n",
    "\n",
    "Since ${\\bf X}$ is a rank one matrix, we can use the nuclear norm to\n",
    "enforce approximately low-rank solutions. Then, we can formulate the\n",
    "blind deconvolution problem as follows:\n",
    "$${\\boldsymbol X}^\\star \\in \\arg \\min_{ {\\boldsymbol X}} \\bigg\\{ \\frac{1}{2} \\| \\mathbf{A}({\\boldsymbol X}) - {\\boldsymbol b}\\|_2^2 :  \\| {\\boldsymbol X}\\|_\\ast \\leq \\kappa, ~{\\boldsymbol X}\\in \\mathbb{R}^{p\\times m}   \\bigg\\}, \\tag{4}\\label{eq:FWform}$$\n",
    "where $\\kappa > 0$ is a tuning parameter.\n",
    "\n",
    "Note that our problem is constrained to the nuclear norm ball\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: {\\boldsymbol X}\\in \\mathbb{R}^{p \\times m} , \\| {\\boldsymbol X}\\|_\\ast \\leq \\kappa \\}$.\n",
    "\n",
    "We will apply the Frank-Wolfe algorithm to solve the optimization\n",
    "problem given in \\eqref{eq:FWform}. The Frank-Wolfe algorithm is one of the earliest\n",
    "algorithms that avoids projections. Instead of projections, it leverages\n",
    "lmos (for a very good survey see [Jaggi2013](https://proceedings.mlr.press/v28/jaggi13.html)):\n",
    "$$\\mathrm{lmo}(\\nabla f ({\\boldsymbol Z})) = \\arg \\min_{{\\boldsymbol X}\\in \\mathcal{X}} ~ \\langle \\nabla f ({\\boldsymbol Z}), {\\boldsymbol X}\\rangle,$$\n",
    "where\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: \\| {\\boldsymbol X}\\|_\\ast \\leq \\kappa, ~{\\boldsymbol X}\\in \\mathbb{R}^{p\\times m} \\}$\n",
    "as in Part 1. It applies to the generic constrained minimization\n",
    "template with a smooth objective function,\n",
    "$\\min_{\\boldsymbol X}\\{ f({\\boldsymbol X}) : {\\boldsymbol X}\\in \\mathcal{X}, \\, \\mathcal{X} \\text{ - convex, compact}  \\}$\n",
    "as follows:\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Frank-Wolfe's algorithm\n",
    "\n",
    "1. Choose ${\\boldsymbol X}^0 \\in\\mathcal{X}$.\n",
    "\n",
    "2. For $k=0, 1, \\ldots$ perform:\n",
    "  $$\\begin{cases}\n",
    "  \\hat{{\\bf X}}^k &:= \\mathrm{lmo}(\\nabla f ({\\boldsymbol X}^k)), \\\\\n",
    "  {\\bf X}^{k+1} &:= (1-\\gamma_k){\\bf X}^k + \\gamma_k\\hat{{\\bf X}}^k,\n",
    "  \\end{cases}$$ where $\\gamma_k := {2}/{(k+2)}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 1.4.1 (4 pts)\n",
    "\n",
    "Recall that the Frank-Wolfe algorithm applies only for\n",
    "    smooth objectives. Show that the objective function is smooth in the sense its gradient is Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d723d8",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "Fill your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740f510",
   "metadata": {},
   "source": [
    "### Implementation of Frank-Wolfe\n",
    "\n",
    "Complete the missing lines of the Frank-Wolfe update below.\n",
    "We provide you the linear operators that you need to compute the LMO in the code. Note that we do not need to\n",
    "    store and use the linear operator ${\\boldsymbol A}$ in the ambient\n",
    "    dimensions. In fact, for storage and arithmetic efficiency, we\n",
    "    should avoid explicitly writing ${\\boldsymbol A}$. You can find more\n",
    "    details about this aspect as comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe2f3e-bf73-4639-b7ac-5513b647944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf29bb-e52b-4487-83a9-59fab8eb953d",
   "metadata": {},
   "source": [
    "We will track three states:\n",
    "\n",
    "- `X`: corresponding to $\\boldsymbol X$\n",
    "- `AX`: corresponding to $\\boldsymbol{AX}$\n",
    "- `k`: the iterate count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744d821-7abd-416e-85b5-9a5681e2d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state():\n",
    "    return OptState(X=0.0, AX=0.0, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcc19e-fd17-4389-939f-c982dd66de95",
   "metadata": {},
   "source": [
    "#### Question 1.4.2 (2 points) Complete the gradient computation of $f(\\boldsymbol X)$\n",
    "You have access to the following operators and variables:\n",
    "    \n",
    "- `A_T(Z)`: Computes $\\boldsymbol A^\\top \\boldsymbol Z$ for some $\\boldsymbol Z$.\n",
    "- `Aoper`: Computes $\\boldsymbol A \\boldsymbol X$ for 1-rank matrix $\\boldsymbol X$ using its SVD decomposition.\n",
    "- `b`: corresponds to $\\boldsymbol b$\n",
    "\n",
    "**Remark**: The `grad` method will receive `AX` and not `X` for computational efficiency reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3362772-7cdb-46ff-ac56-943c03c021aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Function(\n",
    "    f = lambda AX: 0.5*np.linalg.norm(AX - b, 2)**2,\n",
    "    grad = lambda AX: ???,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011278e-7fc5-445d-8f3e-5bb1198421d2",
   "metadata": {},
   "source": [
    "#### Question 1.4.3 (5 points) Complete the LMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4901b8-5bb5-4595-abe0-b3c1d6be4c3e",
   "metadata": {},
   "source": [
    "You are allowed to use `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84a1ef-d64b-4e4c-a549-ef113280a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmo(Grad, kappa=1000):\n",
    "    \"\"\" This function implements the lmo operator.\n",
    "    \"\"\"\n",
    "\n",
    "    Xhat = ???\n",
    "\n",
    "    # Apply A to the rank 1 update\n",
    "    AXhat = Aoper(topLe_vec, -kappa, topRe_vec.T)\n",
    "\n",
    "    return (Xhat, AXhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb97b-f35e-40f2-8167-48f84ccdeb07",
   "metadata": {},
   "source": [
    "#### Question 1.4.4 (5 points) Complete the Frank-Wolfe update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a511f65-d0c3-447d-a493-af784f2f9d00",
   "metadata": {},
   "source": [
    "Fill in the missing update of `X` using the LMO from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd96d1-4ea0-41b1-a857-3276c9466c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(f, state):\n",
    "    X, AX, k = state\n",
    "\n",
    "    Xhat, AXhat = ???\n",
    "\n",
    "    # Step size\n",
    "    gamma_k = ???\n",
    "\n",
    "    # Update X\n",
    "    next_X = ???\n",
    "\n",
    "    # Update A*X\n",
    "    next_AX = (1.0-gamma_k)*AX + gamma_k*(AXhat)\n",
    "\n",
    "    return OptState(next_X, next_AX, k+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46707353-179a-437e-b222-b1ad09786e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_algorithm = OptAlgorithm(\"FrankWolfe\", init_state, state_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd7055-e5cd-443d-91bb-2628c05d46e9",
   "metadata": {},
   "source": [
    "#### Question 1.4.5 (3 points) Run Frank-Wolfe\n",
    "\n",
    "Tune the \n",
    "    parameter $\\kappa$ until the license plate number becomes readable.\n",
    "    What is the license plate number? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Frank-Wolfe's method\n",
    "xFW = run_frank_wolfe(f, opt_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eb9b5",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "Fill your answer here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476f9cb-6950-46fe-9af4-82774ca25d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
