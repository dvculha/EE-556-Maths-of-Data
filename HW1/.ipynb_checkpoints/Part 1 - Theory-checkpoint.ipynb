{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you worked with somebody else on this Homework, fill in their names here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hector Ramirez - We discussed the theory questions and compared the plots of the application parts after we have finished coding implementation by ourselves. \n",
    "\n",
    "I also asked the TA a couple of questions for Part 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 10 Points\n",
    "\n",
    "\n",
    "We consider a binary classification task that we will model using logistic regression. Your goal will be to find a classifier using first-order methods and accelerated gradient descent methods. The first part will consist of more theoretical questions, and the second and third part will ask you to implement these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classic approach to _binary classification_. Before we dive in, let us first define the standard logistic function $\\sigma$ on which most of what follows is built:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma : x \\mapsto \\frac{1}{1 + \\exp{(-x)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In logistic regression, we model the _conditional probability_ of observing a class label $b$ given a set of features $\\mathbf{a}$. More formally, if we observe $n$ independent samples\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{a}_i\\in\\mathbb{R}^p$ and $b_i\\in\\{0, 1\\}$ is the class label, we _assume_ that $b_i$ given $\\mathbf{a}_i$ is a Bernouilli random variable with parameter $\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)$, for some unknown $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$. In other words, we assume that there exists an $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = 1 \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural) \\quad \\text{ and } \\quad \\mathbb{P}(b_i = 0 \\mid \\mathbf{a}_i) = 1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)=  \\sigma( - \\mathbf{a}_i^T\\mathbf{x}^\\natural).\n",
    "\\end{equation*}\n",
    "\n",
    "This is our statistical model. It can be written in a more compact form as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)^j\\sigma(-\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{1-j}, \\quad j \\in \\{0, 1\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal now is to determine the unknown $\\mathbf{x}^\\natural$ by constructing an estimator.\n",
    "\n",
    "We are provided with a set of $n$ independent observations, we can write down the negative log-likelihood $f$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tf(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n)) & = -\\log \\left( \\prod_{i=1}^{n} \\mathbb{P}(b_i|a_i) \\right) \\quad \\quad \\text{(by independence)}\\\\\n",
    "\t\t & =  \\sum_{i=1}^n - \\log(\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{b_i}\\sigma(-\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{1-b_i}) \\\\\n",
    "\t\t & = \\sum_{i=1}^n  b_i \\log(1 + \\exp(- \\mathbf{a}_i^T\\mathbf{x})) + (1-b_i)\\log(1 + \\exp(\\mathbf{a}_i^T\\mathbf{x})).\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "__(a)__ (1 point) Show that the function $u \\mapsto \\log(1 + \\exp(u))$ is convex. Deduce that $f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A twice differentiable function f(x) is convex if and only if it's Hessian is greater or equal to 0 and only if its domain is convex. This can be interpreted as an upward curvature at x.\n",
    "\n",
    "$\\frac{d^2f(u)}{du^2} \\succcurlyeq 0; \\forall u \\in dom(f)$\n",
    "\n",
    "Function is:\n",
    "\n",
    "$f(u) = \\log(1 + \\exp(u))$\n",
    "\n",
    "First derivative is:\n",
    "\n",
    "$\\frac{df(u)}{du} = \\frac{e^u}{1 + e^u}$\n",
    "\n",
    "Second derivative is:\n",
    "\n",
    "$\\frac{df^2(u)}{du^2} =  \\frac{e^u}{(1 + e^u)^2}$\n",
    "\n",
    "It is known that:\n",
    "\n",
    "$e^x \\geq 0 \\forall u \\in \\mathbb{R}$\n",
    "\n",
    "Therefore,\n",
    "$\\frac{e^u}{(1 + e^u)^2} \\geq 0; \\forall u \\in \\mathbb{R} $\n",
    "\n",
    "And,\n",
    "$\\sum_{i=1}^n  b_i \\log(1 + \\exp(- \\mathbf{a}_i^T\\mathbf{x})) + (1-b_i)\\log(1 + \\exp(\\mathbf{a}_i^T\\mathbf{x})) \\in \\mathbb C$ since additions and convex combinations of convex sets are convex.\n",
    "\n",
    "Thus, $dom(f) \\in \\mathbb C$ \n",
    "\n",
    "Therefore, this shows $f(u)$ is a twice differentiable function and it is convex since it's Hessian is greater or equal to 0 for any given x and domain of the function is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator, which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^\\star_{ML} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "is a global minimum so it can serve as an estimator of $\\mathbf{x}^\\natural$. But, does the minimum always exist? We will ponder this question in the following three points.\n",
    "\n",
    "__(b)__ (1 point) Explain the difference between infima and minima.  Give an example of a convex function, defined over $\\mathbb{R}$, that does not attain its infimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the function with the lowest value is the minima of the function. The largest bound where every other point in the function is greater than or equal to is the infima of the function. \n",
    "\n",
    "$f: \\mathbb{R} \\rightarrow \\mathbb{R}:  f(x) = e^{x}$ is an example of a convex function which doesn't attain its minimum since its domain is not bounded and so the function doesn't attain its infimum although it is bounded below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (1 points) Let us assume that there exists $\\mathbf{x}_0 \\in \\mathbb{R}^p$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad \\text{ if } b_i = 1, \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 > 0, \\text{ and if } b_i = 0 \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 < 0.\n",
    "\\end{equation*}\n",
    "\n",
    "This is called _complete separation_ in the literature. Can you think of a geometric reason why this name is appropriate? Think of a 2D example where this can happen (i.e $p=2$) and describe why _complete separation_ is an appropriate name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept can be visualised when p = 2 in $\\mathbb R^2$. If $b_i = 0$, then $\\mathbf{a}_i^T\\mathbf{x}_0 > 0$ and if $b_i = 1$, then $\\mathbf{a}_i^T\\mathbf{x}_0 < 0$. These 2 halfspaces are separated from each other with the line (or hyperplace) of the form $H = ${$x: \\mathbf{a}_i^T\\mathbf{x}_0 = 0$}. Therefore the name complete separation is appropriate when it is thought geometrically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that our data satisfies the complete separation assumption. Unfortunately, as you will show in the following question, this can become an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (2 points) In a _complete separation_ setting, i.e, there exists $\\mathbf{x}_0$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad \\text{ if } b_i = 1, \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 > 0, \\text{ and if } b_i = 0 \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 < 0.\n",
    "\\end{equation*}\n",
    "\n",
    "prove that the function $f$ does not attain its minimum. \n",
    "\n",
    "__Hint__: If the function did have a minimum, would it be above, below or equal to zero? Then think of how $f(2 \\mathbf{x}_0)$ compares with $f(\\mathbf{x}_0)$, how about $f(\\alpha \\mathbf{x}_0)$ for $\\alpha \\rightarrow + \\infty$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum, derivative needs to equal 0.\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = 0 = \\sum_{i=1}^{n}(-\\frac{b_i e^{-a_i^Tx}}{1 + e^{-a_i^Tx}} + \\frac{(1-b_i) e^{a_i^Tx}}{1 + e^{a_i^Tx}})a_i^T$\n",
    "\n",
    "There isn't any x which satisfies this condition since this is always equal to zero. The function is bounded below by zero but it doesn't attain its minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just shown convex functions do not always attain their infimum. So it is possible for the maximum-likelihood estimator $\\mathbf{x}^\\star_{ML}$ to not exist. We will resolve this issue by adding a regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we consider the function\n",
    "\n",
    "\\begin{equation*}\n",
    "\tf_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2\n",
    "\\end{equation*}\n",
    "with $\\mu> 0$.\n",
    "\n",
    "__(e)__ (1 points) Show that the gradient of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n (\\sigma(\\mathbf{a}_i^T\\mathbf{x}) - b_i)\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\tag{1}\\label{gradient}\n",
    "\\end{equation}\n",
    "__Hint__: Lecture 3 shows you how to proceed with this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\sum_{i=1}^n  b_i \\log(1 + \\exp(- \\mathbf{a}_i^T\\mathbf{x})) + (1-b_i)\\log(1 + \\exp(\\mathbf{a}_i^T\\mathbf{x}))$\n",
    "\n",
    "$g(x) = \\frac{\\mu}{2} \\Vert x \\Vert_2^2$. \n",
    "\n",
    "$f_\\mu(x) = f(x) + g(x)$\n",
    "\n",
    "As stated in lecture 3, the Jacobian is the transpose of the gradient, when f is real valued.\n",
    "\n",
    "$J_{f_\\mu}(x) = J_f(x) + J_g(x)$.\n",
    "\n",
    "$J_f(x)$ is computed as:\n",
    "\n",
    "$J_{f(x)} = \\sum_{i=1}^{n}(-\\frac{b_i e^{-a_i^Tx}}{1 + e^{-a_i^Tx}} + \\frac{(1-b_i) e^{a_i^Tx}}{1 + e^{a_i^Tx}})a_i^T$\n",
    "\n",
    "\n",
    "$J_{g(x)}$ is computed as:\n",
    "\n",
    "$J_{g(x)} = 2 \\frac{\\mu}{2} x^T = \\mu x^T$\n",
    "\n",
    "$J_{f_\\mu}(x) = \\sum_{i=1}^{n}(\\sigma(a_i^Tx)-b_i)a_i^T + \\mu x^T$\n",
    "\n",
    "$\\nabla f_\\mu(x) = [J_{{f_\\mu}(x)}]^T$\n",
    "\n",
    "$\\nabla f_\\mu(x) = (\\sum_{i=1}^{n}(\\sigma(a_i^Tx)-b_i)a_i^T + \\mu x^T)^T $\n",
    "\n",
    "This yields the expression above.\n",
    "\n",
    "$\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n (\\sigma(\\mathbf{a}_i^T\\mathbf{x}) - b_i)\\mathbf{a}_i + \\mu \\mathbf{x}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (1 points) Show that the Hessian of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "\\tag{2}\\label{eq:hessian}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2$\n",
    "\n",
    "\n",
    "$ \\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n \\sigma(\\mathbf{a}_i^T\\mathbf{x}) - b_i)\\mathbf{a}_i + \\mu \\mathbf{x}$\n",
    "\n",
    "Differentiate the above wrt x using chain rule,\n",
    "\n",
    "$ \\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n (\\sigma(\\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}))a_ia_i^T + \\mu I$\n",
    "\n",
    "And since $\\sigma(-x) = (1-\\sigma(x))$, the above can be rewritten as\n",
    "\n",
    "$ \\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}$\n",
    "\n",
    "This yields the equation in the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to observe that we can write the Hessian in a more compact form by defining the matrix\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "It is easy to see that we have\n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) =  \\mathbf{A}^T \\text{Diag}\\left( \\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x}) \\right)\\mathbf{A}+ \\mu \\mathbf{I}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A twice differentiable function $f_\\mu(x)$ is $\\mu$-strongly convex if and only if it's Hessian is greater or equal to $\\mu I$. \n",
    "\n",
    "$\\nabla^2f_\\mu(x) \\succcurlyeq \\mu I; \\forall x \\in dom(f_\\mu)$\n",
    "\n",
    "$\\nabla^2f_\\mu(x) = \\nabla^2f(x) + \\mu I $\n",
    "\n",
    "Therefore to show $f_\\mu(x)$ is $\\mu$-strongly convex, one can show f(x) is twice differentiable and convex.\n",
    "\n",
    "$\\nabla^2f(x) = \\mathbf{A}^T Diag(\\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{A}$\n",
    "\n",
    "As in part (a), $\\nabla^2f(x) \\succcurlyeq  0;  \\forall x \\in dom(f)$.\n",
    "\n",
    "This shows $f_\\mu(x)$ is indeed $\\mu$-strongly convex.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Is it possible for a strongly convex function, defined over $\\mathbb{R}^p$, to not attain its minimum ? <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Justify your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. As per definition of strong convexity, \n",
    "\n",
    "$f(y) \\geq f(x) + \\nabla f(x)^T(y - x) + \\frac{\\mu}{2} \\Vert (y - x) \\Vert_2^2; \\forall x,y \\in dom(f)$\n",
    "\n",
    "$\\nabla^2f(x) \\succcurlyeq  0$\n",
    "\n",
    "Second equation implies upward curvature and first equation implies that for any point x in dom(f), the linear approximation f(x) is always going to lie above or on the linear approximation plus the quadratic term $\\frac{\\mu}{2} \\Vert (y - x) \\Vert_2^2$\n",
    "\n",
    "This shows, strongly convex functions are bounded below and there exists one local minimum only, which is also the global minimum. Therefore, it is not possible for a strongly convex function defined over $\\mathbb{R}^p$, to not attain its minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e, $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left\\| a_i\\right\\|_2^2 =  a_i^T a_i = Tr(a_i a_i^T)$\n",
    "\n",
    "$a_i^T a_i$ is a 1x1 matrix therefore it only has one eigenvalue equal to $(a_i^T a_i)_{1,1} = \\left\\| \\mathbf{a}_i\\right\\|_2^2 \\geq 0$\n",
    "\n",
    "We know trace is the sum of eigenvalues $\\lambda_i$ for matrix $a_i a_i^T$,\n",
    "\n",
    "$\\left\\| a_i\\right\\|_2^2 = \\sum_{i=1}^n\\lambda_i$\n",
    "\n",
    "$a_i^T a_i$ and $a_i a_i^T$ have same non-zero eigenvalues since they have same characteristic polynomials. \n",
    "\n",
    "This means, $a_i a_i^T$ can only have one non-zero eigenvalue equal to $\\left\\| a_i\\right\\|_2^2 \\geq 0$ \n",
    "\n",
    "This is therefore the maximum eigenvalue of $a_i a_i^T$\n",
    "\n",
    "$\\lambda_{\\max}(a_i a_i^T) = \\left\\| a_i\\right\\|_2^2$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-2)__ Using [2](#mjx-eqn-eq2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$. \n",
    "\n",
    "__Hint__: Recall that $\\lambda_{\\max}(\\cdot)$ verifies the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^n \\lambda_{\\max}(\\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^Tx \\mathbf{a}_i \\mathbf{a}_i^T) + \\lambda_{\\max}(\\mu I)$\n",
    "\n",
    "From i-1, $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$\n",
    "\n",
    "$\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^n \\|\\mathbf{a}_i\\|_2^2 \\lambda_{\\max}(\\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^Tx) + \\mu$\n",
    "\n",
    "We know, $0 \\leq (\\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^Tx) \\geq 1$\n",
    "\n",
    "Thus for maximum value, $(\\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^Tx) = 1$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^n \\|\\mathbf{a}_i\\|_2^2 + \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-3__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = \\|A\\|_F^2 + \\mu = \\sum_{i=1}^n \\Vert a_i \\Vert_2^2 + \\mu$\n",
    "\n",
    "From part i-2, it is known that\n",
    "\n",
    "$\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu = L$\n",
    "\n",
    "This means maximum value the eigenvalue of $\\nabla^2 f_\\mu(\\mathbf{x})$ is $L$. Therefore,\n",
    "\n",
    "$\\nabla^2 f_\\mu(\\mathbf{x}) \\preccurlyeq LI$\n",
    "\n",
    "Therefore, it can be concluded that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From your work in this section, you have shown that the maximum likelihood estimator for logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_2^2$ regularizer. Consequently, the estimator for $\\mathbf{x}^\\natural$ we will use will be the solution of the smooth strongly convex problem,\n",
    "\\begin{equation}\n",
    "\t\\mathbf{x}^\\star=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2.\n",
    "\\tag{3}\\label{eq:log_problem}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) TA's will give you candy if you provide a complete proof."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
