{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - 50 points\n",
    "\n",
    "Please review Lectures 10-11 for the notation and the setup in this exercise.\n",
    "\n",
    "As a basic example for the implementation of GANs, we will try to learn the density of a 2D mixture of Gaussian (MoG) distribution from empirical samples using the Wasserstein GAN (WGAN) framework. MoG distributions are multi-modal, and allow to us visualize multiple \"modes\" of the distribution. \n",
    "\n",
    "Both your generator neural network $h_\\mathbf{x}$ and dual variable neural network $\\mathtt{d}_\\mathbf{y}$ are defined as two hidden layer networks:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{H}:=\\{h: h_{\\mathbf{x}}(\\omega)=X_3\\texttt{relu}(X_2\\texttt{relu}(X_1 \\omega + x_1)+ x_2) + x_3\\},\n",
    "    \\qquad \\mathcal{D}:= \\{\\mathtt{d}: \\mathtt{d}_{\\mathbf{x}}({\\bf a})=\n",
    "    Y_3\\texttt{relu}(Y_2\\texttt{relu}(Y_1 {\\bf a} + y_1)+ y_2)+y_3\\},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x} =[x_1;x_2;{X_1};{X_2}; X_3]$ are the \"generator\" parameters, and $\\mathbf{y} =[y_1;y_2;{Y_1};{Y_2}; Y_3] $ are the\n",
    "\"dual\" or the \"discriminator\" parameters. The dimensions of these parameters will be apparent from the context as well as the base code provided along with the homework. \n",
    "\n",
    "In the following cells, you will implement the two neural networks as well as the spectral normalization ([Myato 2018](https://arxiv.org/abs/1802.05957)) method that enforces a Lipschitz constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** (3 points) Implement a two hidden layer MLP with ReLU below for the generator. See `torch.nn.Sequential` docs for a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, noise_dim=2, output_dim=2, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Using Sequential to create a small model, stack the given layer one after the other\n",
    "        self.inner = nn.Sequential(nn.Linear(noise_dim, hidden_dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_dim, hidden_dim),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_dim, output_dim)\n",
    "                                  )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.inner(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** (20 points) Implement a two hidden layer MLP with ReLU below for the Discriminator and implement the spectral normalization method.\n",
    "\n",
    "**Remark:** The `spectral_normalization` does not need to return a value. It should modify the parameters $\\mathbf{y}$ of the dual network $\\mathtt{d}_{\\mathbf{y}}$ in place. Remember not to track gradients in those operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linalg as LA\n",
    "\n",
    "class DualVariable(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=2, hidden_dim=100, c=1e-2):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        \n",
    "        # Using Sequential to create a small model, stack the given layer one after the other\n",
    "        self.inner = nn.Sequential(\n",
    "          nn.Linear(input_dim, hidden_dim),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_dim, hidden_dim),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "\n",
    "    def enforce_lipschitz(self):\n",
    "        self.spectral_normalisation()\n",
    "\n",
    "    def spectral_normalisation(self):\n",
    "        \"\"\"\n",
    "        Perform spectral normalisation, forcing the singular value of the weights to be upper bounded by 1.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.named_parameters():\n",
    "                \n",
    "                if not \"weight\" in name:\n",
    "                    continue\n",
    "                    \n",
    "                #for param in self.named_parameters():    \n",
    "                # modify the parameters by dividing with the spectral norm (the 2-norm)\n",
    "                # torch.linalg.matrix_norm() computes a matrix norm.\n",
    "                norm = LA.matrix_norm(param, ord=2)\n",
    "                if norm > 1:\n",
    "                    param = param / norm\n",
    "                    \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** (2 points) Spectral Normalization applied to a matrix outputs a matrix whose largest singular value is upper bounded by one. Is Spectral Normalization a projection (in the $\\ell_2$ sense) ? Think of a diagonal square matrix and try figure out what spectral normalization does and compare that to what a projection onto the set of matrices with largest singular value less than 1 would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral norm of a matrix A is the largest singular value of A (i.e., the square root of the largest eigenvalue of the matrix $A^\\star A$). It corresponds to the greatest distortion of the unit circle or sphere. Spectral normalisation divides each element by the spectral norm. \n",
    "\n",
    "Let $A = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "The largest eigenvalue of A is 4. Therefore\n",
    "\n",
    "$A_{spectral-normalised} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{bmatrix}$\n",
    "\n",
    "Now, the new matrix is a matrix whose largest singular value is upper bounded by one.\n",
    "\n",
    "Spectral normalisation is not a projection in the $\\ell_2$ sense.\n",
    "\n",
    "$A_{spectral-normalised}^2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1/16 \\end{bmatrix} \\neq A_{spectral-normalised} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{bmatrix}$\n",
    "\n",
    "Idempotence property of projection does not hold, therefore spectral normalisation is not a projection in the $\\ell_2$ sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** (1 point) Implement a stochastic estimate of the objective function of the minimax game (in the cell below):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} \n",
    "\\mathbb{E}[\\mathtt{d}_{\\mathbf{y}}({\\bf a})] - \\mathbb{E}[\\mathtt{d}_{\\mathbf{y}}(h_{\\mathbf{x}}(\\omega))] = \n",
    "\\min_{h \\in \\mathcal{H}} \\max_{\\mathtt{d} \\in \\mathcal{F}}\n",
    "\\mathbb{E}[\\mathtt{d}({\\bf a})] - \\mathbb{E}[\\mathtt{d}(h(\\omega))],\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(d, g, data_sample, noise_sample):\n",
    "    # h = g\n",
    "    #E_d_a = torch.mean(d(data_sample))\n",
    "    #E_d_h_w = torch.mean(d(g(noise_sample)))\n",
    "    E_d_a = (d(data_sample)).mean()\n",
    "    E_d_h_w = (d(g(noise_sample))).mean()\n",
    "    W1 = E_d_a - E_d_h_w\n",
    "    return W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** (20 points) Implement an alternating gradient ascent/descent update, training the generator 1 time for every 5 dual updates. More specifically, you will implement the conceptual algorithm below using the parameters of the neural networks\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathtt{d}^{k+1} &= \\text{EnforceLipschitz} (\\mathtt{d}^k + \\gamma \\text{SG}_\\mathtt{d}(\\mathtt{d}^k, h^kk)), && \\text{(if $k$ mod $5 \\neq 0$) }\\\\\n",
    "h^{k+1} &= h^{k} - \\gamma \\text{SG}_h(\\mathtt{d}^{k+1}, h^k) && \\text{(if $k$ mod $5 =0$)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{SG}$ is the stochastic gradient oracle. To perform the optimization, you have two `Pytorch` optimizers `d_optim` and `g_optim`, which have a `.step()` method that updates the discriminator and generator parameters respectively.\n",
    "\n",
    "Use the objective function you have just written. The iteration count is held by the `step_k` argument. The agument `d_ratio` defines how many more times we train the discriminator that train the generator. \n",
    "\n",
    "For later, to display the progress, write the function so that it returns the value of the objective function.\n",
    "\n",
    "**Hints**: Don't forget that the generator seeks to minimize and the discriminator seeks to maximize. Pytorch optimizers step in the _negative_ gradient direction, keep that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_update(step_k, d, g, d_optim, g_optim, noise_samples, real_samples, d_ratio=5):\n",
    "    d_optim.zero_grad()\n",
    "    g_optim.zero_grad()\n",
    "    \n",
    "    l = objective(d, g, real_samples, noise_samples)\n",
    "    \n",
    "    if step_k % d_ratio==0:\n",
    "        d.eval()\n",
    "        g.train()\n",
    "        min_loss = l # generator seeks to minimize\n",
    "        min_loss.backward()\n",
    "        with torch.no_grad():\n",
    "            g_optim.step()\n",
    "            #g_optim.zero_grad()\n",
    "    else:\n",
    "        d.train()\n",
    "        g.eval()\n",
    "        max_loss = l #-l discriminator seeks to maximize\n",
    "        max_loss.backward()\n",
    "        with torch.no_grad():\n",
    "            d_optim.step()\n",
    "            d.enforce_lipschitz()\n",
    "            #d_optim.zero_grad()\n",
    "            \n",
    "    # recalculate loss\n",
    "    l = objective(d, g, real_samples, noise_samples)\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the ingredients in hand, we can train our GAN.\n",
    "\n",
    "The following cell defines the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100 #increase this\n",
    "d = DualVariable(input_dim=2, hidden_dim=hidden_dim, c=1.0)\n",
    "g = Generator(noise_dim=2, output_dim=2, hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** (2 points) Define an optimizer for each of the networks. We recommend choosing `Adam` with betas $(0.0, 0.9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01 #decrease this\n",
    "d_optim = torch.optim.Adam(d.parameters(), lr = step_size, betas = (0.0, 0.9), maximize = True)\n",
    "g_optim = torch.optim.Adam(g.parameters(), lr = step_size, betas = (0.0, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the training loop for 2000 iterations, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1:-4.3e+01 GTrue: 100%|██████████| 2000/2000 [08:36<00:00,  3.87it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(d, g, d_optim, g_optim, alternating_update, n_iter=2000) # and increase this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is complete, you can visualize the training process in the following gif. (If you encounter problems seeing the gif, open it in a separate tab, it can be found in the same directory as this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"movie.gif\")\n",
    "\n",
    "#Note - I had to delete the output of this cell, otherwise, it is too large to upload to Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** (2 points) Briefly comment on what you observe. Play with the network sizes, the step sizes, the number of iterations, all while keeping in mind the difficulties of min-max optimization you've analyzed in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook 1, we saw that solving min-max problems could be extremely challenging. They come with difficulties such as scalability, mode collapse, periodic cycling. In this notebook, it is observed that when the step size (learning rate) is larger, the model is diverged and unable to be trained successfully since it gets stuck. Therefore, a small enough step size is required for the training algorithm to converge. Moreover, a large enough network size is needed. When the learning rate is large and the number of hidden dimensions isn't large enough, the algorithm doesn't converge, no matter the number of iterations. And it required a much larger number of iterations to acquire good results. When I ran it with 0.001 step size, 400 hidden layers and 10 000 iterations, very satisfactory results were reached. These results are presented in a compressed gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
